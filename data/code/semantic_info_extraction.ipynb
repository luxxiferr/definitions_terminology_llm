{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "10750f36-1293-4108-8804-a701b4f4e0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando términos: 77it [00:00, 77208.08it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [13], line 125\u001b[0m\n\u001b[1;32m    123\u001b[0m contexts \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    124\u001b[0m definition \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdefinition\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m--> 125\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[43mrow\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msource\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m()\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m definition:\n\u001b[1;32m    127\u001b[0m     contexts\u001b[38;5;241m.\u001b[39mappend(definition)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIGURACIÓN ===\n",
    "BASE_DIR = \"/home/jovyan/lrec_2026/data_10/info_data\"\n",
    "CSV_INPUT = os.path.join(BASE_DIR, \"glossary_info_data_with_definitions_es_100.csv\")\n",
    "CSV_OUTPUT = os.path.join(BASE_DIR, \"glossary_info_data_info_semantica_es.csv\")\n",
    "JSON_CACHE = os.path.join(BASE_DIR, \"iate_info_data_100_es.json\")\n",
    "\n",
    "TARGET_DOMAIN = \"EDUCATION AND COMMUNICATIONS\"\n",
    "SUBDOMAIN_KEYWORD = \"information technology and data processing\"\n",
    "\n",
    "# Idioma a usar: \"en\" o \"es\"\n",
    "LANG = \"es\"  \n",
    "\n",
    "# === AUXILIARES ===\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)  # quitar HTML\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalizar espacios\n",
    "    return text.strip()\n",
    "\n",
    "def filter_item_from_iate(item, target_domain=TARGET_DOMAIN, subdomain_keyword=SUBDOMAIN_KEYWORD, lang=LANG):\n",
    "    \"\"\"Extrae info semántica completa desde un item de IATE en el idioma indicado\"\"\"\n",
    "    # --- Dominios ---\n",
    "    domains = [d.get(\"domain\", {}).get(\"name\", \"\").strip() \n",
    "               for d in item.get(\"domains\", []) \n",
    "               if d.get(\"domain\", {}).get(\"name\")]\n",
    "    domain_ok = any(\n",
    "        target_domain.lower() in d.lower() and (subdomain_keyword.lower() in d.lower() if subdomain_keyword else True)\n",
    "        for d in domains\n",
    "    )\n",
    "    if not domain_ok:\n",
    "        return None\n",
    "\n",
    "    # --- Elegir idioma ---\n",
    "    lang_entries = item.get(\"language\", {}).get(lang)\n",
    "    if not lang_entries and lang != \"en\":\n",
    "        # fallback a inglés si no hay datos en el idioma elegido\n",
    "        lang_entries = item.get(\"language\", {}).get(\"en\")\n",
    "\n",
    "    if not lang_entries:\n",
    "        return None  # no hay datos de idioma\n",
    "\n",
    "    # --- Variantes ---\n",
    "    variants = []\n",
    "    for te in lang_entries.get(\"term_entries\", []):\n",
    "        term_val = te.get(\"term_value\", \"\").strip()\n",
    "        if term_val:\n",
    "            variants.append(term_val)\n",
    "\n",
    "    # --- Contextos, referencias, uso y notas ---\n",
    "    contexts = []\n",
    "    for te in lang_entries.get(\"term_entries\", []):\n",
    "        # Contextos principales\n",
    "        for ctx in te.get(\"contexts\", []):\n",
    "            text = clean_text(ctx.get(\"context\", \"\"))\n",
    "            ref_text = ctx.get(\"reference\", {}).get(\"text\", \"\").strip()\n",
    "            combined = f\"{text} | Ref: {ref_text}\" if ref_text else text\n",
    "            if combined:\n",
    "                contexts.append(combined)\n",
    "        # Term references\n",
    "        for ref in te.get(\"term_references\", []):\n",
    "            ref_text = clean_text(ref.get(\"text\", \"\"))\n",
    "            if ref_text:\n",
    "                contexts.append(f\"Ref: {ref_text}\")\n",
    "        # Language usage\n",
    "        usage = te.get(\"language_usage\", {}).get(\"value\", \"\").strip()\n",
    "        if usage:\n",
    "            contexts.append(f\"Usage: {usage}\")\n",
    "        # Notas\n",
    "        note_val = te.get(\"note\", {}).get(\"value\", \"\").strip()\n",
    "        if note_val:\n",
    "            contexts.append(f\"Note: {note_val}\")\n",
    "\n",
    "    # --- Construir info_semantica ---\n",
    "    info_parts = []\n",
    "    if variants:\n",
    "        info_parts.append(f\"Variants: {', '.join(sorted(set(variants)))}\")\n",
    "    if domains:\n",
    "        info_parts.append(f\"Domains: {', '.join(domains)}\")\n",
    "    if contexts:\n",
    "        info_parts.append(f\"Context: {' | '.join(contexts)}\")\n",
    "\n",
    "    return {\"info_semantica\": \" | \".join(info_parts) if info_parts else f\"Domains: {TARGET_DOMAIN}\"}\n",
    "\n",
    "# === LEER CACHE JSON ===\n",
    "if os.path.exists(JSON_CACHE):\n",
    "    with open(JSON_CACHE, \"r\", encoding=\"utf-8\") as f:\n",
    "        iate_cache = json.load(f)\n",
    "else:\n",
    "    iate_cache = {}\n",
    "\n",
    "# === LEER CSV Y PROCESAR ===\n",
    "rows_out = []\n",
    "with open(CSV_INPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\";\")\n",
    "    fieldnames = list(reader.fieldnames) + [\"info_semantica\"]\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Procesando términos\"):\n",
    "        # Eliminar claves None\n",
    "        row = {k: v for k, v in row.items() if k is not None}\n",
    "\n",
    "        term = row.get(\"term\", \"\").strip()\n",
    "        if not term:\n",
    "            continue\n",
    "\n",
    "        sem_info = None\n",
    "        cached = iate_cache.get(term)\n",
    "        if cached:\n",
    "            for item in cached.get(\"items\", []):\n",
    "                sem_info = filter_item_from_iate(item, lang=LANG)\n",
    "                if sem_info:\n",
    "                    break\n",
    "\n",
    "        # Si no hay info en la cache, usamos CSV\n",
    "        if not sem_info:\n",
    "            variants = [term] if term else []\n",
    "            contexts = []\n",
    "            definition = row.get(\"definition\", \"\").strip()\n",
    "            source = row.get(\"source\", \"\").strip()\n",
    "            if definition:\n",
    "                contexts.append(definition)\n",
    "            if source:\n",
    "                contexts.append(f\"Ref: {source}\")\n",
    "            info_parts = []\n",
    "            if variants:\n",
    "                info_parts.append(f\"Variants: {', '.join(variants)}\")\n",
    "            info_parts.append(f\"Domains: {TARGET_DOMAIN}\")\n",
    "            if contexts:\n",
    "                info_parts.append(f\"Context: {' | '.join(contexts)}\")\n",
    "            sem_info = {\"info_semantica\": \" | \".join(info_parts)}\n",
    "\n",
    "        # Solo actualizar claves conocidas\n",
    "        for k, v in sem_info.items():\n",
    "            if k and k in fieldnames:\n",
    "                row[k] = v\n",
    "\n",
    "        rows_out.append(row)\n",
    "\n",
    "# === GUARDAR CSV FINAL ===\n",
    "with open(CSV_OUTPUT, \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
    "    writer = csv.DictWriter(out, fieldnames=fieldnames, delimiter=\";\")\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows_out)\n",
    "\n",
    "# === GUARDAR CACHE JSON ===\n",
    "with open(JSON_CACHE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(iate_cache, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ CSV final guardado en: {CSV_OUTPUT} (Idioma: {LANG})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9c6830de-c04b-4127-a850-1dff709b411c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Procesando términos: 100it [00:00, 81269.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ CSV final guardado en: /home/jovyan/lrec_2026/data_10/defence/glossary_defence_info_semantica_en.csv (Idioma: en)\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "# === CONFIGURACIÓN ===\n",
    "BASE_DIR = \"/home/jovyan/lrec_2026/data_10/defence\"\n",
    "CSV_INPUT = os.path.join(BASE_DIR, \"glossary_defence_with_definitions_en_100.csv\")\n",
    "CSV_OUTPUT = os.path.join(BASE_DIR, \"glossary_defence_info_semantica_en.csv\")\n",
    "JSON_CACHE = os.path.join(BASE_DIR, \"iate_defence_100_en.json\")\n",
    "\n",
    "TARGET_DOMAIN = \"INTERNATIONAL RELATIONS\"\n",
    "SUBDOMAIN_KEYWORD = \"defence\"\n",
    "\n",
    "# Idioma a usar: \"en\" o \"es\"\n",
    "LANG = \"en\"  \n",
    "\n",
    "# === AUXILIARES ===\n",
    "def clean_text(text):\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    text = re.sub(r\"<[^>]+>\", \"\", text)  # quitar HTML\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # normalizar espacios\n",
    "    return text.strip()\n",
    "\n",
    "def filter_item_from_iate(item, target_domain=TARGET_DOMAIN, subdomain_keyword=SUBDOMAIN_KEYWORD, lang=LANG):\n",
    "    \"\"\"Extrae info semántica completa desde un item de IATE en el idioma indicado\"\"\"\n",
    "    # --- Dominios ---\n",
    "    domains = [d.get(\"domain\", {}).get(\"name\", \"\").strip() \n",
    "               for d in item.get(\"domains\", []) \n",
    "               if d.get(\"domain\", {}).get(\"name\")]\n",
    "    domain_ok = any(\n",
    "        target_domain.lower() in d.lower() and (subdomain_keyword.lower() in d.lower() if subdomain_keyword else True)\n",
    "        for d in domains\n",
    "    )\n",
    "    if not domain_ok:\n",
    "        return None\n",
    "\n",
    "    # --- Elegir idioma ---\n",
    "    lang_entries = item.get(\"language\", {}).get(lang)\n",
    "    if not lang_entries and lang != \"en\":\n",
    "        # fallback a inglés si no hay datos en el idioma elegido\n",
    "        lang_entries = item.get(\"language\", {}).get(\"en\")\n",
    "\n",
    "    if not lang_entries:\n",
    "        return None  # no hay datos de idioma\n",
    "\n",
    "    # --- Variantes ---\n",
    "    variants = []\n",
    "    for te in lang_entries.get(\"term_entries\", []):\n",
    "        term_val = te.get(\"term_value\", \"\").strip()\n",
    "        if term_val:\n",
    "            variants.append(term_val)\n",
    "\n",
    "    # --- Contextos, referencias, uso y notas ---\n",
    "    contexts = []\n",
    "    for te in lang_entries.get(\"term_entries\", []):\n",
    "        # Contextos principales\n",
    "        for ctx in te.get(\"contexts\", []):\n",
    "            text = clean_text(ctx.get(\"context\", \"\"))\n",
    "            ref_text = ctx.get(\"reference\", {}).get(\"text\", \"\").strip()\n",
    "            combined = f\"{text} | Ref: {ref_text}\" if ref_text else text\n",
    "            if combined:\n",
    "                contexts.append(combined)\n",
    "        # Term references\n",
    "        for ref in te.get(\"term_references\", []):\n",
    "            ref_text = clean_text(ref.get(\"text\", \"\"))\n",
    "            if ref_text:\n",
    "                contexts.append(f\"Ref: {ref_text}\")\n",
    "        # Language usage\n",
    "        usage = te.get(\"language_usage\", {}).get(\"value\", \"\")\n",
    "        if usage:\n",
    "            contexts.append(f\"Usage: {usage.strip()}\")\n",
    "        # Notas\n",
    "        note_val = te.get(\"note\", {}).get(\"value\", \"\")\n",
    "        if note_val:\n",
    "            contexts.append(f\"Note: {note_val.strip()}\")\n",
    "\n",
    "    # --- Construir info_semantica ---\n",
    "    info_parts = []\n",
    "    if variants:\n",
    "        info_parts.append(f\"Variants: {', '.join(sorted(set(variants)))}\")\n",
    "    if domains:\n",
    "        info_parts.append(f\"Domains: {', '.join(domains)}\")\n",
    "    if contexts:\n",
    "        info_parts.append(f\"Context: {' | '.join(contexts)}\")\n",
    "\n",
    "    return {\"info_semantica\": \" | \".join(info_parts) if info_parts else f\"Domains: {TARGET_DOMAIN}\"}\n",
    "\n",
    "# === LEER CACHE JSON ===\n",
    "if os.path.exists(JSON_CACHE):\n",
    "    with open(JSON_CACHE, \"r\", encoding=\"utf-8\") as f:\n",
    "        iate_cache = json.load(f)\n",
    "else:\n",
    "    iate_cache = {}\n",
    "\n",
    "# === LEER CSV Y PROCESAR ===\n",
    "rows_out = []\n",
    "with open(CSV_INPUT, \"r\", encoding=\"utf-8\") as f:\n",
    "    reader = csv.DictReader(f, delimiter=\";\")\n",
    "    fieldnames = list(reader.fieldnames) + [\"info_semantica\"]\n",
    "\n",
    "    for row in tqdm(reader, desc=\"Procesando términos\"):\n",
    "        # Eliminar claves None\n",
    "        row = {k: v for k, v in row.items() if k is not None}\n",
    "\n",
    "        term = (row.get(\"term\") or \"\").strip()\n",
    "        if not term:\n",
    "            continue\n",
    "\n",
    "        sem_info = None\n",
    "        cached = iate_cache.get(term)\n",
    "        if cached:\n",
    "            for item in cached.get(\"items\", []):\n",
    "                sem_info = filter_item_from_iate(item, lang=LANG)\n",
    "                if sem_info:\n",
    "                    break\n",
    "\n",
    "        # Si no hay info en la cache, usamos CSV\n",
    "        if not sem_info:\n",
    "            variants = [term] if term else []\n",
    "            contexts = []\n",
    "            definition = (row.get(\"definition\") or \"\").strip()\n",
    "            source = (row.get(\"source\") or \"\").strip()\n",
    "            if definition:\n",
    "                contexts.append(definition)\n",
    "            if source:\n",
    "                contexts.append(f\"Ref: {source}\")\n",
    "            info_parts = []\n",
    "            if variants:\n",
    "                info_parts.append(f\"Variants: {', '.join(variants)}\")\n",
    "            info_parts.append(f\"Domains: {TARGET_DOMAIN}\")\n",
    "            if contexts:\n",
    "                info_parts.append(f\"Context: {' | '.join(contexts)}\")\n",
    "            sem_info = {\"info_semantica\": \" | \".join(info_parts)}\n",
    "\n",
    "        # Solo actualizar claves conocidas\n",
    "        for k, v in sem_info.items():\n",
    "            if k and k in fieldnames:\n",
    "                row[k] = v\n",
    "\n",
    "        rows_out.append(row)\n",
    "\n",
    "# === GUARDAR CSV FINAL ===\n",
    "with open(CSV_OUTPUT, \"w\", encoding=\"utf-8\", newline=\"\") as out:\n",
    "    writer = csv.DictWriter(out, fieldnames=fieldnames, delimiter=\";\")\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rows_out)\n",
    "\n",
    "# === GUARDAR CACHE JSON ===\n",
    "with open(JSON_CACHE, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(iate_cache, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"✅ CSV final guardado en: {CSV_OUTPUT} (Idioma: {LANG})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393b4a0a-cc1b-4b07-8a7d-30272f41fd1f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
