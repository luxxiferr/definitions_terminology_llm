{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebe76f2-190d-4f25-80cb-82ed4a1948ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG with Nomic + FAISS + Llama 3.1 \n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import faiss\n",
    "from tqdm import tqdm\n",
    "from nomic import embed\n",
    "from transformers import pipeline\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# ------------------ CONFIG ------------------\n",
    "HF_TOKEN = \"hf_token\"  # tu token de Hugging Face\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "MODEL_EMBEDDINGS = \"nomic-embed-text-v1.5\"\n",
    "MODEL_LLM = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
    "\n",
    "DATA_PATH = \"/home/jovyan/lrec_2026/data_10/UN/glossary_UN_info_semantica_es.csv\"\n",
    "OUTPUT_PATH = \"/home/jovyan/lrec_2026/data_10/UN/UN_rag_es_llama.csv\"\n",
    "\n",
    "LIMIT_TERMS = 100\n",
    "TOP_K = 3\n",
    "\n",
    "# ------------------ CARGA CSV ------------------\n",
    "print(\"üì• Cargando CSV...\")\n",
    "df = pd.read_csv(DATA_PATH, sep=\";\", quotechar='\"', engine=\"python\", on_bad_lines=\"skip\")\n",
    "print(f\"‚úÖ CSV cargado con {len(df)} filas y columnas: {list(df.columns)}\")\n",
    "\n",
    "if \"term\" not in df.columns or \"info_semantica\" not in df.columns:\n",
    "    raise ValueError(\"‚ùå El CSV debe tener las columnas 'term' y 'info_semantica'.\")\n",
    "\n",
    "# Limitar cantidad de t√©rminos (para pruebas)\n",
    "df = df.head(LIMIT_TERMS)\n",
    "\n",
    "# ------------------ LIMPIEZA ------------------\n",
    "def clean_html(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    return BeautifulSoup(str(text), \"html.parser\").get_text(separator=\" \", strip=True)\n",
    "\n",
    "df[\"info_semantica\"] = df[\"info_semantica\"].apply(clean_html).fillna(\"\")\n",
    "df[\"term\"] = df[\"term\"].astype(str).fillna(\"\")\n",
    "\n",
    "# Crear documentos combinando t√©rmino e info sem√°ntica\n",
    "documents = [\n",
    "    f\"T√©rmino: {t}\\nContexto sem√°ntico: {c}\" for t, c in zip(df[\"term\"], df[\"info_semantica\"])\n",
    "]\n",
    "\n",
    "print(f\"üìò Procesando {len(documents)} documentos para embeddings...\")\n",
    "\n",
    "# ------------------ EMBEDDINGS NOMICS ------------------\n",
    "def get_nomic_embeddings(texts, model=MODEL_EMBEDDINGS):\n",
    "    \"\"\"Devuelve los embeddings (vectores) de una lista de textos\"\"\"\n",
    "    res = embed.text(texts=texts, model=model, task_type=\"search_document\")\n",
    "    return np.array(res[\"embeddings\"], dtype=\"float32\")\n",
    "\n",
    "def l2_normalize(a, axis=1, eps=1e-10):\n",
    "    norms = np.linalg.norm(a, axis=axis, keepdims=True)\n",
    "    return a / (norms + eps)\n",
    "\n",
    "print(\"üîπ Generando embeddings con Nomic...\")\n",
    "embeddings = get_nomic_embeddings(documents)\n",
    "embeddings = l2_normalize(embeddings)\n",
    "print(f\"‚úÖ Embeddings generados con forma: {embeddings.shape}\")\n",
    "\n",
    "# ------------------ INDEX FAISS ------------------\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)  # inner product ‚âà similitud coseno\n",
    "index.add(embeddings)\n",
    "print(\"‚úÖ √çndice FAISS creado y poblado\")\n",
    "\n",
    "# ------------------ FUNCI√ìN DE RECUPERACI√ìN ------------------\n",
    "def rag_retrieve(term, info_semantica, top_k=TOP_K, include_self=True):\n",
    "    \"\"\"\n",
    "    Recupera los documentos m√°s parecidos usando embeddings.\n",
    "    include_self=True permite incluir el propio documento.\n",
    "    \"\"\"\n",
    "    query_text = f\"T√©rmino: {term}\\nContexto sem√°ntico: {info_semantica}\"\n",
    "    q_emb = get_nomic_embeddings([query_text])\n",
    "    q_emb = l2_normalize(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    retrieved = [documents[i] for i in I[0]]\n",
    "\n",
    "    if not include_self:\n",
    "        retrieved = [d for d in retrieved if not d.startswith(f\"T√©rmino: {term}\\n\")]\n",
    "\n",
    "    return retrieved\n",
    "\n",
    "# ------------------ MODELO LLM ------------------\n",
    "print(\"üîπ Cargando modelo Llama 3.1...\")\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_LLM,\n",
    "    tokenizer=MODEL_LLM,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    max_new_tokens=256,\n",
    "    temperature=0.5,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "print(\"‚úÖ Modelo cargado correctamente\")\n",
    "\n",
    "# ------------------ PROMPT ------------------\n",
    "PROMPT_TEMPLATE = \"\"\"Eres un experto en terminolog√≠a.\n",
    "Bas√°ndote √∫nicamente en la siguiente informaci√≥n sem√°ntica, redacta una definici√≥n precisa y concisa en espa√±ol para el t√©rmino: \"{term}\".\n",
    "No inventes informaci√≥n; si la informaci√≥n no es suficiente, escribe \"No hay informaci√≥n suficiente para definir el t√©rmino.\"\n",
    "\n",
    "Informaci√≥n sem√°ntica (contexto recuperado):\n",
    "{context}\n",
    "\n",
    "Definici√≥n:\n",
    "\"\"\"\n",
    "\n",
    "# ------------------ GENERAR DEFINICIONES ------------------\n",
    "def generate_definition(term, info_semantica, top_k=TOP_K):\n",
    "    retrieved = rag_retrieve(term, info_semantica, top_k=top_k, include_self=True)\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved).strip() or \"No hay informaci√≥n sem√°ntica disponible.\"\n",
    "\n",
    "    prompt = PROMPT_TEMPLATE.format(term=term, context=context)\n",
    "    output = llm_pipeline(prompt)[0][\"generated_text\"]\n",
    "\n",
    "    if \"Definici√≥n:\" in output:\n",
    "        definition = output.split(\"Definici√≥n:\")[-1].strip()\n",
    "    else:\n",
    "        definition = output.strip()\n",
    "\n",
    "    return definition.split(\"\\n\")[0].strip()\n",
    "\n",
    "# ------------------ PROCESAMIENTO ------------------\n",
    "definitions = []\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"üß† Generando definiciones (RAG)\"):\n",
    "    try:\n",
    "        definition = generate_definition(row[\"term\"], row[\"info_semantica\"], top_k=TOP_K)\n",
    "    except Exception as e:\n",
    "        definition = f\"ERROR: {e}\"\n",
    "    definitions.append(definition)\n",
    "\n",
    "df[\"definition\"] = definitions\n",
    "\n",
    "df.to_csv(OUTPUT_PATH, sep=\";\", index=False)\n",
    "print(f\"\\n‚úÖ Archivo final guardado en: {OUTPUT_PATH}\")\n",
    "print(f\"üìò T√©rminos procesados: {len(df)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
