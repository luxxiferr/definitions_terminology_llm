{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b34ef8ee-60a0-4231-86a2-86a53ea85587",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "RAG pipeline robusto (solo URLs -> scraping -> embeddings Nomic -> FAISS -> LLM)\n",
    "- Usa SOLO URLs en columna `source`.\n",
    "- Scraping robusto + limpieza HTML.\n",
    "- Embeddings por batch con reintentos (gestiona 502).\n",
    "- FAISS IndexFlatIP sobre vectores L2-normalizados (cosine sim).\n",
    "- Generaci√≥n de definiciones por batch con un pipeline de transformers.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "import faiss\n",
    "from transformers import pipeline\n",
    "from nomic import embed\n",
    "import torch\n",
    "import gc\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util import Retry\n",
    "\n",
    "# ---------------- CONFIG ----------------\n",
    "HF_TOKEN = \"hf_token\" \n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HF_TOKEN\n",
    "\n",
    "MODEL_EMBEDDINGS = \"nomic-embed-text-v1.5\"\n",
    "MODEL_LLM = \"mistralai/Mistral-7B-v0.1\" \n",
    "\n",
    "CSV_INPUT = \"/home/jovyan/lrec_2026/data_10/civil_law/glossary_civil_law_info_semantica_es.csv\"\n",
    "OUTPUT_CSV = \"/home/jovyan/lrec_2026/RAG/celex_rag_definitions_civil_law_mistral.csv\"\n",
    "\n",
    "# Ajustables seg√∫n recursos\n",
    "SCRAPE_TIMEOUT = 12\n",
    "SCRAPE_RETRIES = 3\n",
    "SCRAPE_BACKOFF = 1.5\n",
    "\n",
    "EMB_BATCH_SIZE = 8         \n",
    "EMB_RETRIES = 4\n",
    "EMB_BACKOFF = 2.0\n",
    "\n",
    "TOP_K = 3\n",
    "LLM_BATCH_SIZE = 4        \n",
    "OUTPUT_LANG = \"spanish\"\n",
    "\n",
    "# ---------------- UTILIDADES ----------------\n",
    "def make_requests_session(retries=3, backoff_factor=0.5, status_forcelist=(429, 500, 502, 503, 504)):\n",
    "    s = requests.Session()\n",
    "    retry = Retry(total=retries, read=retries, connect=retries,\n",
    "                  backoff_factor=backoff_factor, status_forcelist=status_forcelist,\n",
    "                  allowed_methods=False)  # allow all methods\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    s.mount('http://', adapter)\n",
    "    s.mount('https://', adapter)\n",
    "    # headers b√°sicos (imitamos navegador)\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0 Safari/537.36\",\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "        \"Accept-Language\": \"es-ES,es;q=0.9,en;q=0.8\"\n",
    "    })\n",
    "    return s\n",
    "\n",
    "session = make_requests_session(retries=SCRAPE_RETRIES, backoff_factor=SCRAPE_BACKOFF)\n",
    "\n",
    "def clean_text(text):\n",
    "    if text is None:\n",
    "        return \"\"\n",
    "    # eliminar m√∫ltiples espacios, tabs y newlines\n",
    "    text = re.sub(r'\\s+', ' ', str(text))\n",
    "    return text.strip()\n",
    "\n",
    "def extract_urls_from_field(source_field):\n",
    "    # extrae urls http/https; devuelve lista o []\n",
    "    if pd.isna(source_field):\n",
    "        return []\n",
    "    urls = re.findall(r'https?://[^\\s\"\\'<>]+', str(source_field))\n",
    "    # limpiar caracteres finales comunes\n",
    "    clean_urls = []\n",
    "    for u in urls:\n",
    "        u = u.strip().rstrip('.,;)\">')\n",
    "        clean_urls.append(u)\n",
    "    return list(dict.fromkeys(clean_urls))  # deduplicar manteniendo orden\n",
    "\n",
    "def is_pdf_response(resp):\n",
    "    ctype = resp.headers.get('Content-Type', '').lower()\n",
    "    return 'pdf' in ctype\n",
    "\n",
    "def get_main_text_from_html(html):\n",
    "    \"\"\"\n",
    "    Heur√≠stica para extraer el texto \"principal\":\n",
    "    - preferir <article>, <main>\n",
    "    - si no, buscar el contenedor con mayor suma de longitud de <p>\n",
    "    - limpiar nav/footer, scripts, estilos\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for tag in soup([\"script\", \"style\", \"noscript\", \"header\", \"footer\", \"nav\", \"svg\", \"img\", \"iframe\"]):\n",
    "        tag.decompose()\n",
    "    # prefer <article> or <main>\n",
    "    article = soup.find('article')\n",
    "    if article:\n",
    "        txt = article.get_text(separator=' ', strip=True)\n",
    "        if len(txt) > 100:\n",
    "            return clean_text(txt)\n",
    "    main = soup.find('main')\n",
    "    if main:\n",
    "        txt = main.get_text(separator=' ', strip=True)\n",
    "        if len(txt) > 100:\n",
    "            return clean_text(txt)\n",
    "    # fallback: buscar el contenedor con mayor cantidad de texto en <p>\n",
    "    candidates = soup.find_all(['div', 'section', 'body'], recursive=True)\n",
    "    best_text = \"\"\n",
    "    best_len = 0\n",
    "    for c in candidates:\n",
    "        ps = c.find_all('p')\n",
    "        if not ps:\n",
    "            continue\n",
    "        combined = \" \".join(p.get_text(separator=' ', strip=True) for p in ps)\n",
    "        l = len(combined)\n",
    "        if l > best_len:\n",
    "            best_len = l\n",
    "            best_text = combined\n",
    "    if best_len >= 80:\n",
    "        return clean_text(best_text)\n",
    "    # √∫ltimo recurso: todo el body\n",
    "    body = soup.get_text(separator=' ', strip=True)\n",
    "    return clean_text(body)\n",
    "\n",
    "def scrape_url(url, session=session, timeout=SCRAPE_TIMEOUT):\n",
    "    \"\"\"\n",
    "    Robusto: reintentos gestionados por session. Devuelve texto limpio o None.\n",
    "    Omite PDFs (podr√≠as descargar/ocr si lo necesitas).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        resp = session.get(url, timeout=timeout)\n",
    "    except Exception as e:\n",
    "        return None\n",
    "    if resp is None or resp.status_code != 200:\n",
    "        return None\n",
    "    # ignorar PDFs / binarios\n",
    "    if is_pdf_response(resp):\n",
    "        return None\n",
    "    text = get_main_text_from_html(resp.text)\n",
    "    return text if text and len(text) > 80 else None\n",
    "\n",
    "# ---------------- EMBEDDINGS (NOMIC) con retries por batch ----------------\n",
    "def get_nomic_embeddings_batch(texts, model=MODEL_EMBEDDINGS, retries=EMB_RETRIES, backoff=EMB_BACKOFF):\n",
    "    \"\"\"\n",
    "    texts: list[str]\n",
    "    devuelve np.array(shape=(len(texts), dim), dtype=float32) o lanza excepci√≥n.\n",
    "    Implementa retries exponenciales en caso de fallos 502/5xx.\n",
    "    \"\"\"\n",
    "    attempt = 0\n",
    "    while attempt <= retries:\n",
    "        try:\n",
    "            res = embed.text(texts=texts, model=model, task_type=\"search_document\")\n",
    "            emb = np.array(res[\"embeddings\"], dtype=\"float32\")\n",
    "            return emb\n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            # intentar si es un error transitorio\n",
    "            wait = backoff ** attempt\n",
    "            print(f\"‚ö†Ô∏è Embeddings error (attempt {attempt}/{retries}). Retrying in {wait:.1f}s. Error: {e}\")\n",
    "            time.sleep(wait)\n",
    "    raise RuntimeError(f\"Failed to get embeddings after {retries} retries.\")\n",
    "\n",
    "def l2_normalize(a, axis=1, eps=1e-10):\n",
    "    norms = np.linalg.norm(a, axis=axis, keepdims=True)\n",
    "    return a / (norms + eps)\n",
    "\n",
    "# ---------------- CARGA CSV y extracci√≥n URLs ----------------\n",
    "print(\"üì• Cargando CSV de entrada...\")\n",
    "df = pd.read_csv(CSV_INPUT, sep=';', encoding='utf-8', engine='python', on_bad_lines='skip')\n",
    "\n",
    "# Extraer URLs de 'source' (solo URLs v√°lidas)\n",
    "df['urls_scrapables'] = df['source'].apply(extract_urls_from_field)\n",
    "df_scrap = df[df['urls_scrapables'].map(len) > 0].reset_index(drop=True)\n",
    "print(f\"‚úÖ Filas con URLs scrapables: {len(df_scrap)}\")\n",
    "\n",
    "# ---------------- SCRAPING y colecci√≥n de documentos (por t√©rmino) ----------------\n",
    "all_docs = []       # textos (strings)\n",
    "terms_scrap = []    # t√©rmino correspondiente\n",
    "sources_scrap = []  # lista de urls originales usadas\n",
    "\n",
    "print(\"üìÑ Haciendo scraping por t√©rmino (headers + retries)...\")\n",
    "for idx, row in tqdm(df_scrap.iterrows(), total=len(df_scrap), desc=\"Scraping t√©rminos\"):\n",
    "    term = row['term']\n",
    "    urls = row['urls_scrapables']\n",
    "    term_texts = []\n",
    "    used_urls = []\n",
    "    for url in urls:\n",
    "        try:\n",
    "            t = scrape_url(url)\n",
    "        except Exception:\n",
    "            t = None\n",
    "        if t:\n",
    "            term_texts.append(t)\n",
    "            used_urls.append(url)\n",
    "        # peque√±o sleep para no saturar\n",
    "        time.sleep(0.2)\n",
    "    if term_texts:\n",
    "        # concatenar los textos de las URLs v√°lidas para ese t√©rmino\n",
    "        joined = \"\\n\\n\".join(term_texts)\n",
    "        # truncar para no pasar texto enorme a embeddings (opcional)\n",
    "        MAX_CHARS = 100000\n",
    "        if len(joined) > MAX_CHARS:\n",
    "            joined = joined[:MAX_CHARS]\n",
    "        all_docs.append(joined)\n",
    "        terms_scrap.append(term)\n",
    "        sources_scrap.append(json.dumps(used_urls, ensure_ascii=False))  # para guardar en CSV\n",
    "print(f\"‚úÖ Scraping completado. Documentos v√°lidos obtenidos: {len(all_docs)}\")\n",
    "\n",
    "if len(all_docs) == 0:\n",
    "    raise SystemExit(\"No se obtuvieron documentos scrapeados. Revisa las URLs en tu CSV.\")\n",
    "\n",
    "# ---------------- GENERAR EMBEDDINGS EN BATCHES y construir matriz completa ----------------\n",
    "print(\"üîπ Generando embeddings en batches...\")\n",
    "embeddings_list = []\n",
    "for i in tqdm(range(0, len(all_docs), EMB_BATCH_SIZE), desc=\"Embeddings batches\"):\n",
    "    batch_texts = all_docs[i:i+EMB_BATCH_SIZE]\n",
    "    try:\n",
    "        emb_batch = get_nomic_embeddings_batch(batch_texts)\n",
    "        emb_batch = l2_normalize(emb_batch)\n",
    "        embeddings_list.append(emb_batch)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error en embeddings batch para √≠ndices {i}-{i+len(batch_texts)-1}: {e}\")\n",
    "        # continuar con los que s√≠ funcionaron\n",
    "        continue\n",
    "    # limpieza de memoria\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "if len(embeddings_list) == 0:\n",
    "    raise RuntimeError(\"No se pudieron generar embeddings para ning√∫n documento.\")\n",
    "\n",
    "embeddings = np.vstack(embeddings_list).astype('float32')\n",
    "print(f\"‚úÖ Embeddings generados. Shape: {embeddings.shape}\")\n",
    "\n",
    "# ---------------- FAISS (IndexFlatIP sobre vectores normalizados) ----------------\n",
    "dim = embeddings.shape[1]\n",
    "index = faiss.IndexFlatIP(dim)\n",
    "index.add(embeddings)\n",
    "print(\"‚úÖ √çndice FAISS creado y poblado\")\n",
    "\n",
    "# ---------------- RAG: funci√≥n de recuperaci√≥n ----------------\n",
    "def rag_retrieve_by_term(term, top_k=TOP_K):\n",
    "    # Query text: usar solo t√©rmino (podr√≠as incluir metadata si quieres)\n",
    "    query_text = f\"T√©rmino: {term}\"\n",
    "    q_emb = get_nomic_embeddings_batch([query_text])\n",
    "    q_emb = l2_normalize(q_emb)\n",
    "    D, I = index.search(q_emb, top_k)\n",
    "    retrieved_docs = []\n",
    "    retrieved_sources = []\n",
    "    for idx in I[0]:\n",
    "        if idx < len(all_docs):\n",
    "            retrieved_docs.append(all_docs[idx])\n",
    "            retrieved_sources.append(sources_scrap[idx])\n",
    "    return retrieved_docs, retrieved_sources\n",
    "\n",
    "# ---------------- CARGAR LLM (transformers pipeline) ----------------\n",
    "print(\"üîπ Cargando modelo LLM (transformers pipeline)...\")\n",
    "llm_pipeline = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=MODEL_LLM,\n",
    "    tokenizer=MODEL_LLM,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=\"auto\",\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.2,\n",
    "    repetition_penalty=1.05\n",
    ")\n",
    "print(\"‚úÖ LLM cargado\")\n",
    "\n",
    "# ---------------- PROMPT (Spanish) ----------------\n",
    "PROMPT_TEMPLATE = \"\"\"Eres un experto en terminolog√≠a.\n",
    "Usando √∫nicamente la informaci√≥n sem√°ntica proporcionada en el contexto, escribe una definici√≥n precisa y concisa en espa√±ol para el t√©rmino: \"{term}\".\n",
    "\n",
    "Contexto (documentos recuperados):\n",
    "{context}\n",
    "\n",
    "Definici√≥n:\n",
    "\"\"\"\n",
    "\n",
    "# ---------------- GENERACI√ìN DE DEFINICIONES EN BATCH ----------------\n",
    "print(\"üß† Generando definiciones (RAG + LLM) por batches...\")\n",
    "definitions = []\n",
    "out_terms = []\n",
    "out_sources = []\n",
    "\n",
    "for i in tqdm(range(0, len(terms_scrap), LLM_BATCH_SIZE), desc=\"Generando batches LLM\"):\n",
    "    batch_terms = terms_scrap[i:i+LLM_BATCH_SIZE]\n",
    "    for term in batch_terms:\n",
    "        try:\n",
    "            retrieved_docs, retrieved_sources = rag_retrieve_by_term(term, top_k=TOP_K)\n",
    "            context = \"\\n\\n---\\n\\n\".join(retrieved_docs).strip()\n",
    "            if not context:\n",
    "                context = \"No hay informaci√≥n sem√°ntica disponible.\"\n",
    "            prompt = PROMPT_TEMPLATE.format(term=term, context=context)\n",
    "            out = llm_pipeline(prompt)[0][\"generated_text\"]\n",
    "            # Attempt to cut after the \"Definici√≥n:\" token\n",
    "            if \"Definici√≥n:\" in out:\n",
    "                definition = out.split(\"Definici√≥n:\")[-1].strip()\n",
    "            elif \"Definition:\" in out:\n",
    "                definition = out.split(\"Definition:\")[-1].strip()\n",
    "            else:\n",
    "                # fallback: whole output after the prompt\n",
    "                definition = out.replace(prompt, \"\").strip()\n",
    "            definition = definition.split(\"\\n\")[0].strip()\n",
    "        except Exception as e:\n",
    "            definition = f\"ERROR: {e}\"\n",
    "            retrieved_sources = []\n",
    "        definitions.append(definition)\n",
    "        out_terms.append(term)\n",
    "        out_sources.append(json.dumps(retrieved_sources, ensure_ascii=False))\n",
    "    # limpiar memoria entre batches\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception:\n",
    "        pass\n",
    "    gc.collect()\n",
    "\n",
    "# ---------------- GUARDAR CSV ----------------\n",
    "df_out = pd.DataFrame({\n",
    "    \"term\": out_terms,\n",
    "    \"definition\": definitions,\n",
    "    \"retrieved_sources\": out_sources\n",
    "})\n",
    "\n",
    "df_out.to_csv(OUTPUT_CSV, sep=';', index=False, encoding='utf-8', quoting=csv.QUOTE_ALL, escapechar='\\\\')\n",
    "print(f\"\\n‚úÖ Definiciones generadas y guardadas en: {OUTPUT_CSV}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
